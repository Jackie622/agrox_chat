import streamlit as st
from langchain_openai import ChatOpenAI
import os
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnableBranch, RunnablePassthrough
import sys
import io
import time
import wave
import requests
import re
from tqdm import tqdm
from gradio_client import Client, handle_file
from PIL import Image
import speech_recognition as sr
import threading

# å°†çˆ¶ç›®å½•æ”¾å…¥ç³»ç»Ÿè·¯å¾„ä¸­ï¼Œå¯æ ¹æ®å®é™…è·¯å¾„è°ƒæ•´
# sys.path.append(r'D:\My_Files\å®éªŒå®¤å­¦ä¹ ç›¸å…³\å¤§æ¨¡å‹éƒ¨ç½²')

# ---------------------------------
# è¯­éŸ³å½•åˆ¶æ¨¡å—ï¼ˆå«å€’è®¡æ—¶æç¤ºï¼‰
# ---------------------------------
class AudioRecorder:
    def __init__(self, rate=16000, timeout_seconds=20):
        self.rate = rate
        self.timeout_seconds = timeout_seconds
        self.recognizer = sr.Recognizer()

    def record(self):
        with sr.Microphone(sample_rate=self.rate) as source:
            st.info(f"è¯·åœ¨å€’è®¡æ—¶ç»“æŸå‰è¯´è¯ï¼ˆæœ€é•¿ {self.timeout_seconds} ç§’ï¼‰")
            # UI å ä½
            countdown_text = st.empty()
            progress_bar = st.progress(0)

            # åå°çº¿ç¨‹è´Ÿè´£å®é™…å½•éŸ³
            audio_container = {}
            def _listen():
                try:
                    audio_container["audio"] = self.recognizer.listen(
                        source,
                        timeout=None,                     # ç­‰å¾…é¦–å¥ï¼Œä¸è¶…æ—¶
                        phrase_time_limit=self.timeout_seconds  # æœ€é•¿å½•åˆ¶æ—¶é—´
                    )
                except Exception:
                    pass

            t = threading.Thread(target=_listen, daemon=True)
            t.start()

            # å‰å°æ›´æ–°å€’è®¡æ—¶ä¸è¿›åº¦
            for elapsed in range(self.timeout_seconds):
                if not t.is_alive():
                    break
                remaining = self.timeout_seconds - elapsed
                countdown_text.info(f"å€’è®¡æ—¶å‰©ä½™ï¼š{remaining} ç§’")
                progress_bar.progress((elapsed + 1) / self.timeout_seconds)
                time.sleep(1)

            # æ¸…ç† UI
            countdown_text.empty()
            progress_bar.empty()

            # åˆ¤æ–­æ˜¯å¦æœ‰å½•éŸ³
            audio_data = audio_container.get("audio", None)
            if audio_data is None:
                st.warning("æœªæ£€æµ‹åˆ°è¯­éŸ³è¾“å…¥æˆ–å½•éŸ³è¶…æ—¶")
                return None

            return audio_data.get_wav_data()

    def save_wav(self, wav_bytes: bytes, filename="temp_output.wav"):
        with wave.open(filename, "wb") as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(self.rate)
            wav_file.writeframes(wav_bytes)

    def run(self):
        wav_bytes = self.record()
        if wav_bytes:
            fname = "temp_output.wav"
            self.save_wav(wav_bytes, fname)
            return fname
        return None

    def save_wav(self, wav_bytes: bytes, filename="temp_output.wav"):
        with wave.open(filename, "wb") as wav_file:
            wav_file.setnchannels(1)
            wav_file.setsampwidth(2)
            wav_file.setframerate(self.rate)
            wav_file.writeframes(wav_bytes)

    def run(self):
        wav_bytes = self.record()
        if wav_bytes:
            fname = "temp_output.wav"
            self.save_wav(wav_bytes, fname)
            return fname
        return None


# ---------------------------------
# è¯­éŸ³è¯†åˆ«å™¨ï¼Œä¼˜å…ˆ Gradioï¼Œå¤±è´¥åæç¤º
# ---------------------------------
class SpeechRecognizer:
    def __init__(self, gradio_url="http://127.0.0.1:7860/"):
        try:
            self.client = Client(gradio_url)
        except Exception as e:
            self.client = None
            st.warning(f"æ— æ³•è¿æ¥ Gradio ASR æœåŠ¡: {e}")

    def recognize(self, file_path: str):
        if not self.client:
            st.error("ASR æœåŠ¡æœªè¿æ¥ï¼Œè¯·å¯åŠ¨ Gradio æœåŠ¡æˆ–é…ç½® ASR APIã€‚")
            return None
        try:
            result = self.client.predict(
                input_wav=handle_file(file_path),
                language="zh",
                api_name="/model_inference"
            )
            return result
        except Exception as e:
            st.error(f"è¯­éŸ³è¯†åˆ«å¤±è´¥: {e}")
            return None

# ---------------------------------
# LangChain æ£€ç´¢é—®ç­”æ¨¡å—
# ---------------------------------
def get_retriever():
    from langchain_community.embeddings import ZhipuAIEmbeddings
    from langchain_chroma import Chroma
    embedding = ZhipuAIEmbeddings(model="embedding-3", dimensions=2048)
    persist_directory = r'chroma'
    vectordb = Chroma(persist_directory=persist_directory, embedding_function=embedding)
    return vectordb.as_retriever()


def combine_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs["context"])


def get_qa_history_chain():
    retriever = get_retriever()
    llm = ChatOpenAI(
        openai_api_key=os.getenv("DEEPSEEK_API_KEY"),
        base_url="https://api.deepseek.com/v1",
        model_name="deepseek-chat",
        temperature=1.0
    )
    condense_sys = (
        "è¯·æ ¹æ®èŠå¤©è®°å½•æ€»ç»“ç”¨æˆ·æœ€è¿‘çš„é—®é¢˜ï¼Œ" \
        "å¦‚æœæ²¡æœ‰å¤šä½™çš„èŠå¤©è®°å½•åˆ™è¿”å›ç”¨æˆ·çš„é—®é¢˜ã€‚"
    )
    condense_prompt = ChatPromptTemplate([
        ("system", condense_sys),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ])
    retrieve_docs = RunnableBranch(
        (lambda x: not x.get("chat_history", False), (lambda x: x["input"]) | retriever),
        condense_prompt | llm | StrOutputParser() | retriever,
    )
    system_prompt = (
        "ä½ æ˜¯ä¸€ä¸ªå†œåœºåŠ©æ‰‹ã€‚è¯·ä½¿ç”¨æ£€ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ç‰‡æ®µå›ç­”è¿™ä¸ªé—®é¢˜ã€‚"
        "è¯·ä½¿ç”¨ç®€æ´çš„è¯è¯­å›ç­”ç”¨æˆ·ã€‚\n\n{context}"  # å¦‚æœä½ ä¸çŸ¥é“ç­”æ¡ˆå°±è¯´ä¸çŸ¥é“ã€‚
    )
    qa_prompt = ChatPromptTemplate.from_messages([
        ("system", system_prompt),
        ("placeholder", "{chat_history}"),
        ("human", "{input}"),
    ])
    qa_chain = (
        RunnablePassthrough().assign(context=combine_docs)
        | qa_prompt
        | llm
        | StrOutputParser()
    )
    qa_history_chain = RunnablePassthrough().assign(context=retrieve_docs).assign(answer=qa_chain)
    return qa_history_chain


def gen_response(chain, user_input, chat_history):
    for res in chain.stream({"input": user_input, "chat_history": chat_history}):
        if "answer" in res:
            yield res["answer"]

# ---------------------------------
# Streamlit åº”ç”¨
# ---------------------------------
def main():
    st.markdown("""
    <style>
        .stImage {text-align: center !important;}
    </style>
    """, unsafe_allow_html=True)
    col1, col2, col3 = st.columns([1,2,1])
    with col2:
        st.image(Image.open("ç™½.png"), width=200)
    st.markdown('### ğŸ§‘â€ğŸŒ¾çŸ¥è€˜å†œä¸šå¤§æ¨¡å‹')

    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "qa_chain" not in st.session_state:
        st.session_state.qa_chain = get_qa_history_chain()

    recorder = AudioRecorder()
    recognizer = SpeechRecognizer()
    messages = st.container()

    for role, text in st.session_state.messages:
        with messages.chat_message(role):
            st.write(text)

    col_txt, col_voice = st.columns([4,1])
    user_input = None
    if prompt := col_txt.chat_input("è¾“å…¥é—®é¢˜æˆ–ä½¿ç”¨è¯­éŸ³â€¦"):
        user_input = prompt
    if col_voice.button("ğŸ™ï¸ è¯­éŸ³è¾“å…¥"):
        wav_path = recorder.run()
        if wav_path:
            st.info("æ­£åœ¨è¯†åˆ«è¯­éŸ³â€¦")
            text = recognizer.recognize(wav_path)
            if text:
                user_input = text
                st.info(f"è¯†åˆ«ç»“æœï¼š{text}")
        else:
            st.warning("å½•éŸ³å¤±è´¥ï¼Œè¯·é‡è¯•")

    if user_input:
        st.session_state.messages.append(("human", user_input))
        with messages.chat_message("human"): st.write(user_input)
        answer_stream = gen_response(st.session_state.qa_chain, user_input, st.session_state.messages)
        with messages.chat_message("ai"):
            ai_resp = st.write_stream(answer_stream)
        st.session_state.messages.append(("ai", ai_resp))

if __name__ == "__main__":
    main()
